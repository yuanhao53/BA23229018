---
title: "Homework"
author: "Yuanhao Pu"
date: "2023-12-08"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
# Homework 0

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Example 1: histogram

```{r iris}
library(knitr)
kable(head(iris),row.names = TRUE, align="c")
```

```{r split}
hist(iris$Sepal.Length, main = "Sepal.Length")
hist(iris$Sepal.Width, main = "Sepal.Width")
hist(iris$Petal.Length, main = "Petal.Length")
hist(iris$Petal.Width, main = "Petal.Width")
```

## Example 2: Regression

The least square regression aims at finding a set of parameters $\boldsymbol{\theta}=(\theta_0,\cdots,\theta_n)$ such that,

$$
\min\sum_{i=1}^{n}(h_\boldsymbol{\theta}(x_i)-y_i)^2
$$
where $h_\boldsymbol{\theta}(X)=X_{m\times n}\boldsymbol{\theta}_{n\times 1}$. The solution of $\boldsymbol\theta$ come up with the following form:

$$
\boldsymbol{\theta}=(X^\top X)^{-1}X^\top Y
$$

Consider the regression task on women dataset,
```{r women}
library(knitr)
kable(women,row.names=TRUE,align="c")
```

```{r fit}
fit <- lm(weight~height, data=women)
summary(fit)
plot(women$height,women$weight,xlab="Height",ylab="Weight")
abline(fit)
```

## Example 3: T-test

Suppose two variables $X_1\sim N(\mu_1,\sigma_1^2), X_2\sim N(\mu_2,\sigma_2^2)$ with mean and standard deviation,

$$
\bar{X}_1=\frac{1}{n_1}\sum_{i=1}^{n_1}X_{1i},\ s_1=\sqrt{\frac{1}{n_1-1}\sum_{i=1}^{n_1}(X_{1i}-\bar{X}_1)^2},\\
\bar{X}_2=\frac{1}{n_2}\sum_{i=1}^{n_2}X_{2i},\ s_2=\sqrt{\frac{1}{n_2-1}\sum_{i=1}^{n_2}(X_{2i}-\bar{X}_2)^2}
$$

Due to the definition of t-distribution, we have

$$
\frac{(\bar{X}_1-\bar{X}_2)-(\mu_1-\mu_2)}{s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\sim t(n_1+n_2-2)
$$
where $s_p=\sqrt{\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}}$.

Practically, we apply t-test on the Sepal.Length attribute of the iris dataset,
```{r ttest}
library(knitr)
setosa <- iris[which(iris$Species=='setosa'),]
kable(head(setosa), row.names = TRUE, align = "c")
versicolor <- iris[which(iris$Species=='versicolor'),]
kable(head(versicolor), row.names = TRUE, align = "c")
t.test(setosa$Sepal.Length, versicolor$Sepal.Length)
```

# Homework 1

## In-class exercise

Define a function that satisfies the following requirement:

Apply the inverse transform method to reproduce the function "sample" with (replace=TRUE) ability.


```{r mysample}
# Function Definition
mysample <- function(x, size = NULL, replace=FALSE, prob= NULL){
  if(is.null(size)|(!replace&&size>length(x))){
    size = length(x)
  } #size with (replace=FALSE) should not exceed length(x)
  if(is.null(prob)|(length(prob)!=length(x))){
    prob = rep(1/length(x),length(x))
  } #prob should meet the length with x
  prob = prob/sum(prob)# normalized
  if(replace){
    cp <- cumsum(prob)
    U <- runif(size)
    r <- x[findInterval(U,cp)+1]
    return(r)
  }
  else{
    r <- c()
    for(i in 1:size){
      cp <- cumsum(prob)
      U <- runif(1)
      idx <- findInterval(U,cp)+1
      temp <- x[idx]
      r <- append(r, temp)
      prob <- prob[-idx]
      prob <- prob/sum(prob)
      x <- x[-idx]
    }
    return(r)
  }
}
# Test of Validity
test_sample <- mysample(c(1,5,10,20), size = 1e5, replace = TRUE, prob = 1:4/10)
ct <- as.vector(table(test_sample))
ct/sum(ct)/(1:4/10)
```

```{r Permutation}
#Letters Permutation
test_sample <- mysample(letters, replace = FALSE)
test_sample
```

## Exercise 3.2

The standard Laplace distribution has density $f(x)=\frac{1}{2}e^{−|x|}, x\in\mathbb{R}$.Use the inverse transform method to generate a random sample of size 1000 from this distribution. Use one of the methods shown in this chapter to compare the generated sample to the target distribution.

**Sol:** The CDF of standard Laplace distrbution $P(X\leq x)=\frac{1}{2}[1+\text{sgn}(x)(1-e^{-|x|})]$, thus, the inverse transformation is 

$$
F_X^{-1}(u)=\begin{cases}\log(2u),u\in(0,\frac{1}{2})\\ -\log(2-2u), u\in[\frac{1}{2},1)\end{cases}
$$

# Homework 2

## In-class Exercise

-   Proof that what value $\rho=\frac{l}{d}$ should take to minimize the asymptotic variance of $\hat{\pi}$? ($m\sim B(n, p)$,using $\delta$ method)

-   Take three different values of $\rho$ ($0\leq\rho\leq 1$, including $\rho_{min}$) and use Monte Carlo simulation to verify your answer. ($n = 10^6$ , Number of repeated simulations $K = 100$)

**Sol:** Consider $p=\frac{2l}{\pi d}$, then $\rho=\frac{l}{d}=\frac{\pi p}{2},\mathbb{E}[m]=np$

$$
\hat{\pi}(n)=\frac{2n\rho}{m}=\frac{n\pi p}{m}\overset{Taylor}{\simeq}n\pi p(g(\mathbb{E}[m])-g'(\mathbb{E}[m])(m-\mathbb{E}[m]))\\
Var(\hat{\pi}(n))=(g'(\mathbb{E}[m])^2)Var(m)=\frac{\pi^2}{n^2p^2}*np(1-p)=\frac{\pi^2}{m}(\frac{\pi}{2\rho}-1)
$$

Hence, the asymptotic variance comes to minimum when $\rho$ is set to 1.

```{r}
# Set rho values
rho <- c(0.001,0.5,1)
# Initialize
n <- 1e6
K <- 100
# Storage
var_results <- numeric(length(rho))
# Monte Carlo simulation
for(i in 1:length(rho)){
  rho_value <- rho[i]
  results <- replicate(K, expr = {
    X <- runif(n, 0, 1/2)
    Y <- runif(n, 0, pi/2)
    pi_hat <- 2*rho_value/mean(rho_value/2*sin(Y)>X)
    return(pi_hat)
  })
  var_results[i] <- var(results)
}

data.frame(Rho = rho, Variance = var_results)
```

## Exercise 5.6

In Example 5.7 the control variate approach was illustrated for Monte Carlo integration of

$$
\theta =\int_0^1 e^xdx
$$ Now consider the antithetic variate approach. Compute $Cov(e^U , e^{1−U})$ and $Var(e^U + e^{1−U})$, where $U\sim Uniform(0,1)$. What is the percent reduction in variance of $\hat\theta$ that can be achieved using antithetic variates (compared with simple MC)?

**Sol:**

$$
Cov(e^U,e^{1-U})=\mathbb{E}[e^Ue^{1-U}]-\mathbb{E}[e^U]\mathbb{E}[e^{1-U}]=e-(e-1)^2\\
Var(e^U)=Var(e^{1-U})=\mathbb{E}[e^{2U}]-(\mathbb{E}[e^U])^2=\frac{1}{2}(e^2-1)-(e-1)^2
$$

```{r}
cov <- exp(1)-(exp(1)-1)^2
cov
var <- (exp(2)-1)/2-(exp(1)-1)^2
var

```

Consider the simple MC estimator $\hat\theta_1$ and antithetic estimator $\hat\theta_2$, that is, consider another r.v. $V\sim Uniform(0,1)$ which is i.i.d from $U$, we gain

$$
Var(\hat\theta_1)=Var(\frac{1}{2}(e^U+e^V))=\frac{1}{2}Var(e^U)\\
Var(\hat\theta_2)=Var(\frac{1}{2}(e^U+e^{1-U})) = \frac{1}{2}Var(e^U)+\frac{1}{2}Cov(e^U,e^{1-U})
$$

```{r}
var1 <- var/2
var2 <- (var+cov)/2
percent <- (var1-var2)/var1
percent
```

## Exercise 5.7

Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate $\theta$ by the antithetic variate approach and by the simple Monte Carlo method. Compute an empirical estimate of the percent reduction in variance using the antithetic variate. Compare the result with the theoretical value from Exercise 5.6.

**Sol:**

```{r}
n <- 1e4
rep <- 1e4
# Replicate for variance calculation
simplemc <- replicate(rep, expr ={
  mean(exp(runif(n)))
})
antimc <- replicate(rep, expr = {
  u <- runif(n/2)
  v <- 1-u
  mean((exp(u)+exp(v))/2)
})
mean(simplemc)
mean(antimc)
percent <- (var(simplemc)-var(antimc))/var(simplemc)
percent

```


# Homework 3

## In-class exercise

**Proof:**

Consider $Var(\hat{\theta}^S)=\frac{1}{k^2}\sum_{i=1}^kVar(\hat{\theta}_i)$ and $Var(\hat{\theta}^M)=Var(\hat{\theta}^S)+Var(\theta_I)$,

thus, as $(b_i-a_i)\to0$, $k\to\infty$, $Var(\hat{\theta}^S)/Var(\hat{\theta}^M)=\frac{1}{1+k^2\frac{Var(\theta_I)}{\sum_{i}Var(\hat{\theta}_i)}}\to0$

## Exercise 5.13

Find two importance functions $f_1$ and $f_2$ that are supported on $(1, \infty)$ and are 'close' to

$$
g(x) = \frac{x^2}{\sqrt{2\pi}}\exp(-\frac{x^2}{2})
$$

Which of your two importance functions should produce the smaller variance in estimating

$$
\int_1^\infty\frac{x^2}{\sqrt{2\pi}}\exp(-\frac{x^2}{2})dx
$$

by importance sampling? Explain.

**Sol:**

Let $f_1 = \frac{1}{\sqrt{2\pi}}\exp(-\frac{x^2}{2}),~x>1, ~f_2 = \frac{1}{2}\exp(-\frac{x}{2}),~x>1$

```{r}
m <- 10000
theta.hat <- se <- numeric(2)

g <- function(x){
  x^2/sqrt(2*pi)*exp(-x^2/2)*(x>1)
}

# f1
f1 <- function(x){
  1/sqrt(2*pi)*exp(-x^2/2)
}
x <- rnorm(m)
fg <- g(x)/f1(x)
theta.hat[1] <- mean(fg)
se[1] <- sd(fg)

# f2
f2 <- function(x){
  0.5*exp(-0.5*x)
}
x <- rexp(m,0.5)
fg <- g(x)/f2(x)
theta.hat[2] <- mean(fg)
se[2] <- sd(fg)

table <- data.frame(
  Estimation = theta.hat,Variance = se
)
kable(table)

# Visualization
x <- seq(1.01,10,0.01)
plot(x,g(x), type = 'l', ylab = 'functions')
lines(x,f1(x),lty = 2)
lines(x,f2(x),lty = 3)
legend("topright", legend = c("g(x)","f1(x)","f2(x)"), lty = 1:3)
```

To conclude, $f_2$ computes a smaller variance on variable $Y=g(x)/f(x)$ than $f_1$.

### Exercise 5.14

Obtain a Monte Carlo estimate of

$$
\int_1^\infty\frac{x^2}{\sqrt{2\pi}}\exp(-\frac{x^2}{2})dx
$$

by importance sampling.

**Sol:**

Replicate the above process for 1000 times,

```{r}
m <- 10000
mc1 <- replicate(1000, expr = {
  x <- rnorm(m)
  fg <- g(x)/f1(x)
  return(mean(fg))
})
mc2 <- replicate(1000, expr = {
  x <- rexp(m, 0.5)
  fg <- g(x)/f2(x)
  return(mean(fg))
})
c(mean(mc1), mean(mc2))
c(var(mc1),var(mc2))
```

Intuitively, $f_2$ has a significant decrease than $f_1$ on the variance
of MC estimation. To find out which estimator has a smaller bias, compute

```{r}
true.theta <- integrate(g,lower = 1, upper = Inf)
abs(true.theta$value-mean(mc1))
abs(true.theta$value-mean(mc2))
```

Thus the estimator $f_2$ behaves better than $f_1$.

### Exercise 5.15

Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.

**Sol:**

```{r}
m <- 10000
intervals <- 5
theta.hat <- numeric(intervals)
se <- numeric(intervals)

g <- function(x){
  exp(-x)/(1+x^2)
}
for(j in 1:intervals){
  u <- runif(m, (j-1)/intervals, j/intervals)
  x <- -log(1-(1-exp(-1))*u) #inversion
  f <- function(x){
    5/(1-exp(-1))*exp(-x)
  }
  fg <- g(x)/f(x)
  theta.hat[j] <- mean(fg)
  se[j] <- var(fg)
}

sum(theta.hat)
sqrt(mean(se))

# True value
true.value <- integrate(g,lower = 0, upper = 1)
1 - abs(true.value$value - sum(theta.hat))/abs(true.value$value - 0.5257801)
1 - sqrt(mean(se))/0.0970314
```

Since $f_3(x)$ obtained the estimate $\hat{\theta} =0.5257801$ and standard error $0.0970314$, the stratified estimator performs a lot better with more than 95% shrink in bias and standard error.

### Exercise 6.5

Suppose a 95% symmetric t-interval is applied to estimate a mean, but the sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment to estimate the coverage probability of the $t$-interval for random samples of $\chi^2(2)$ data with sample size $n = 20$. Compare your $t$-interval results with the simulation results in Example 6.4. (The $t$-interval should be more robust to departures from normality than the interval for variance.)

```{r}
n <- 20
root.n <- sqrt(n)
t.interval <- qt(c(0.025,0.975), df = n-1)
CI <- replicate(10000, expr = {
  x <- rchisq(n, df = 2)
  ci <- mean(x) + t.interval * sd(x)/root.n
})
sum(CI[1, ] < 2 & CI[2, ] > 2)
mean(CI[1, ] < 2 & CI[2, ] > 2)
```

which is significantly larger than 77.3\\%.

### Exercise 6.A

Use Monte Carlo simulation to investigate whether the empirical Type I error rate of the $t$-test is approximately equal to the nominal significance level $\alpha$, when the sampled population is non-normal. The t-test is robust to mild departures from normality. Discuss the simulation results for the cases where the sampled population is (i) $\chi^2(1)$, (ii) Uniform(0,2), and (iii) Exponential(rate=1). In each case, test $H_0: \mu=\mu_0$ vs $H_0:\mu\neq\mu_0$, where $\mu_0$ is the mean of $\chi^2(1)$, Uniform(0,2), and Exponential(1), respectively.

```{r}
N <- 10000  
alpha <- 0.05 
mu0 <- 1 


#Chi-Square
df <- 1
type_I_errors <- 0
for (i in 1:N) {
  sample_data <- rchisq(n = 30, df = df)
  t_test <- t.test(sample_data, mu = mu0, alternative = "two.sided")
  if (t_test$p.value < alpha) {
    type_I_errors <- type_I_errors + 1
  }
}
empirical_type_I_error_rate <- type_I_errors / N
cat("Empirical Type I Error Rate with Chi-square(2):",  empirical_type_I_error_rate, "\n")

# Uniform(0,2)
type_I_errors <- 0
for (i in 1:N) {
  sample_data <- runif(n = 30, min = 0, max = 2)
  t_test <- t.test(sample_data, mu = mu0, alternative = "two.sided")
  if (t_test$p.value < alpha) {
    type_I_errors <- type_I_errors + 1
  }
}
empirical_type_I_error_rate <- type_I_errors / N
cat("Empirical Type I Error Rate with Uniform(0,2):",  empirical_type_I_error_rate, "\n")

# Exponential(1)
type_I_errors <- 0
for (i in 1:N) {
  sample_data <- rexp(n = 30, rate = 1)
  t_test <- t.test(sample_data, mu = mu0, alternative = "two.sided")
  if (t_test$p.value < alpha) {
    type_I_errors <- type_I_errors + 1
  }
}
empirical_type_I_error_rate <- type_I_errors / N
cat("Empirical Type I Error Rate with Exponential(1):",  empirical_type_I_error_rate, "\n")
```

# Homework 4

## In-class Exercise

-   Consider $m=1000$ hypotheses, where the first 95% are true to the null hypothesis $H_0$, and the remaining 5% are to the alternative hypothesis $H_1$. $p$-values follow $U(0,1)$ and $Beta(0.1,1)$ respectively for $H_0$ and $H_1$. Apply Bonferroni correction and Benjamini-Hochberg correction to the generated $m$ independent $p$-values (using p.adjust) and obtain the corrected $p$-values. Compare them with $\alpha=0.1$ to determine whether to reject $H_0$. Based on M=1000 simulations, estimate FWER, FDR and TPR.

```{r}
M <- m <- 1000
alpha <- 0.1
FWER1 <- FDR1 <- TPR1 <- numeric(M)
FWER2 <- FDR2 <- TPR2 <- numeric(M)
# M Simulations
for(i in 1:M){
  p_value0 <- runif(0.95*m)
  p_value1 <- rbeta(0.05*m, 0.1, 1)
  
  # Bonferroni adjustment
  adj1 <- p.adjust(c(p_value0, p_value1), method = "bonferroni")
  # B-H
  adj2 <- p.adjust(c(p_value0, p_value1), method = "BH")
  
  result1 <- adj1 > 0.1
  result2 <- adj2 > 0.1
  true_result <- c(rep(TRUE, times = 0.95*m), rep(FALSE, times = 0.05*m))
  
  # All metrics
  FWER1[i] <- sum(result1[0:950]==FALSE)/950
  FWER2[i] <- sum(result2[0:950]==FALSE)/950
  FDR1[i] <- sum(result1[0:950]==FALSE)/sum(result1==FALSE)
  FDR2[i] <- sum(result2[0:950]==FALSE)/sum(result2==FALSE)
  TPR1[i] <- sum(result1[951:1000]==FALSE)/0.05/m
  TPR2[i] <- sum(result2[951:1000]==FALSE)/0.05/m
}

data.frame(FWER = c(sum(FWER1), sum(FWER2)), FDR = c(mean(FDR1),mean(FDR2)), TPR = c(mean(TPR1),mean(TPR2)))

```

The results of the 1000 simulations are in line with what is expected on theoretical understandings.

## Bootstrap Estimation

Suppose the population has the exponential distribution with rate \\lambda, then the MLE of $\lambda$ is $\hat{\lambda}=1/\bar{X}$, where $\bar{X}$ is the sample mean. It can be derived that the expectation of $\hat{\lambda}$ is $\lambda*n/(n-1)$, so that the estimation bias is $\lambda/(n-1)$. The standard error $\hat{\lambda}$ is $\lambda*n/[(n-1)*\sqrt{n-2}]$. Conduct a simulation study to verify the performance of the bootstrap method with:

The true value $\lambda=2$;

The sample size $n=5, 10, 20$;

The number of bootstrap replicates $B=1000$;

The simulations are repeated for $m=1000$ times.

Compare the mean bootstrap bias and bootstrap standard error with the theoretical ones. Comment on the results.

```{r}
library(boot)
library(MASS)

lambda <- 2
sample_size <- c(5,10,20)
B <- 1000
m <- 1000

bias_mean <- numeric(length(sample_size))
se_mean <- numeric(length(sample_size))

# Replicate process
for(n in 1:length(sample_size)){
  bias <- se <- numeric(m)
  for(i in 1:m){
    x <- rexp(sample_size[n], rate = lambda)
    theta <- 1/mean(x)
    thetastar <- numeric(B)
    for(b in 1:B){
      xstar <- sample(x, replace = TRUE)
      thetastar[b] <- 1/mean(xstar)
    }
    bias[i] <- mean(thetastar)-theta
    se[i] <- sd(thetastar)
  }
  bias_mean[n] <- mean(bias)
  se_mean[n] <- mean(se)
}

bias_mean
se_mean
```

```{r}
# Theoretical Results
lambda/(sample_size-1)
lambda*sample_size/(sample_size-1)/sqrt(sample_size-2)
```

It's clear that the empirical means of $bias$ and $se$ are approaching theoretical values as the sample sizes $n$ increase.

## Exercise 7.3

Obtain a bootstrap t confidence interval estimate for the correlation statistic in Example 7.2 (law data in bootstrap).

**Sol:**

```{r}
library(boot)
# apply law dataset
library(bootstrap)
b.cor <- function(x, i) cor(x[i,1], x[i,2])
# return both Cor and Var
b.cor_var <- function(x, i){
  sim <- boot(x[i,], b.cor, R = 100)
  c(sim$t0, var(sim$t)*(n-1)/n^2)
}
b <- boot(law, statistic = b.cor_var, R = 100)
boot.ci(b, type = "stud")

# Unstable
b <- boot(law, statistic = b.cor_var, R = 100)
boot.ci(b, type = "stud")
b <- boot(law, statistic = b.cor_var, R = 100)
boot.ci(b, type = "stud")

```

# Homework 5

## In-class Exercise

-   Consider $m=1000$ hypotheses, where the first 95% are true to the null hypothesis $H_0$, and the remaining 5% are to the alternative hypothesis $H_1$. $p$-values follow $U(0,1)$ and $Beta(0.1,1)$ respectively for $H_0$ and $H_1$. Apply Bonferroni correction and Benjamini-Hochberg correction to the generated $m$ independent $p$-values (using p.adjust) and obtain the corrected $p$-values. Compare them with $\alpha=0.1$ to determine whether to reject $H_0$. Based on M=1000 simulations, estimate FWER, FDR and TPR.

```{r}
M <- m <- 1000
alpha <- 0.1
FWER1 <- FDR1 <- TPR1 <- numeric(M)
FWER2 <- FDR2 <- TPR2 <- numeric(M)
# M Simulations
for(i in 1:M){
  p_value0 <- runif(0.95*m)
  p_value1 <- rbeta(0.05*m, 0.1, 1)
  
  # Bonferroni adjustment
  adj1 <- p.adjust(c(p_value0, p_value1), method = "bonferroni")
  # B-H
  adj2 <- p.adjust(c(p_value0, p_value1), method = "BH")
  
  result1 <- adj1 > 0.1
  result2 <- adj2 > 0.1
  true_result <- c(rep(TRUE, times = 0.95*m), rep(FALSE, times = 0.05*m))
  
  # All metrics
  FWER1[i] <- sum(result1[0:950]==FALSE)/950
  FWER2[i] <- sum(result2[0:950]==FALSE)/950
  FDR1[i] <- sum(result1[0:950]==FALSE)/sum(result1==FALSE)
  FDR2[i] <- sum(result2[0:950]==FALSE)/sum(result2==FALSE)
  TPR1[i] <- sum(result1[951:1000]==FALSE)/0.05/m
  TPR2[i] <- sum(result2[951:1000]==FALSE)/0.05/m
}

data.frame(FWER = c(sum(FWER1), sum(FWER2)), FDR = c(mean(FDR1),mean(FDR2)), TPR = c(mean(TPR1),mean(TPR2)))

```

The results of the 1000 simulations are in line with what is expected on theoretical understandings.

## Bootstrap Estimation

Suppose the population has the exponential distribution with rate \\lambda, then the MLE of $\lambda$ is $\hat{\lambda}=1/\bar{X}$, where $\bar{X}$ is the sample mean. It can be derived that the expectation of $\hat{\lambda}$ is $\lambda*n/(n-1)$, so that the estimation bias is $\lambda/(n-1)$. The standard error $\hat{\lambda}$ is $\lambda*n/[(n-1)*\sqrt{n-2}]$. Conduct a simulation study to verify the performance of the bootstrap method with:

The true value $\lambda=2$;

The sample size $n=5, 10, 20$;

The number of bootstrap replicates $B=1000$;

The simulations are repeated for $m=1000$ times.

Compare the mean bootstrap bias and bootstrap standard error with the theoretical ones. Comment on the results.

```{r}
library(boot)
library(MASS)

lambda <- 2
sample_size <- c(5,10,20)
B <- 1000
m <- 1000

bias_mean <- numeric(length(sample_size))
se_mean <- numeric(length(sample_size))

# Replicate process
for(n in 1:length(sample_size)){
  bias <- se <- numeric(m)
  for(i in 1:m){
    x <- rexp(sample_size[n], rate = lambda)
    theta <- 1/mean(x)
    thetastar <- numeric(B)
    for(b in 1:B){
      xstar <- sample(x, replace = TRUE)
      thetastar[b] <- 1/mean(xstar)
    }
    bias[i] <- mean(thetastar)-theta
    se[i] <- sd(thetastar)
  }
  bias_mean[n] <- mean(bias)
  se_mean[n] <- mean(se)
}

bias_mean
se_mean
```

```{r}
# Theoretical Results
lambda/(sample_size-1)
lambda*sample_size/(sample_size-1)/sqrt(sample_size-2)
```

It's clear that the empirical means of $bias$ and $se$ are approaching theoretical values as the sample sizes $n$ increase.

## Exercise 7.3

Obtain a bootstrap t confidence interval estimate for the correlation statistic in Example 7.2 (law data in bootstrap).

**Sol:**

```{r}
library(boot)
# apply law dataset
library(bootstrap)
b.cor <- function(x, i) cor(x[i,1], x[i,2])
# return both Cor and Var
b.cor_var <- function(x, i){
  sim <- boot(x[i,], b.cor, R = 100)
  c(sim$t0, var(sim$t)*(n-1)/n^2)
}
b <- boot(law, statistic = b.cor_var, R = 100)
boot.ci(b, type = "stud")

# Unstable
b <- boot(law, statistic = b.cor_var, R = 100)
boot.ci(b, type = "stud")
b <- boot(law, statistic = b.cor_var, R = 100)
boot.ci(b, type = "stud")

```

# Homework 6


## Proof of continuous MCMC

Prove the Stationarity of Metropolis-Hastings sampler Algorithm in continuous situation.

**Proof:** Consider the acceptance probability,

$$
\alpha(s,r) = \min(1,\frac{f(r)g(s|r)}{f(s)g(r|s)})
$$

The DBC is trivial for $s=r$, with $s \neq r$, we have

$$
K(s,r)f(s) = g(r|s)\alpha(s,r)f(s)=f(s)g(r|s)\min(1,\frac{f(r)g(s|r)}{f(s)g(r|s)})
$$

Consider the following cases, case 1 with $f(r)g(s|r)<f(s)g(r|s)$, then

$$
K(s,r)f(s) = f(r)g(s|r)\\
K(r,s)f(r) = f(r)g(s|r)
$$

Thus,

$$
K(s,r)f(s) = K(r,s)f(r)
$$

As for case 2 with $f(r)g(s|r)\geq f(s)g(r|s)$, with a similar process,

$$
K(s,r)f(s) = K(r,s)f(r)
$$

## Exercise 8.1

Implement the two-sample Cram´er-von Mises test for equal distributions as a permutation test. Apply the test to the data in Examples 8.1 and 8.2.

```{r}
attach(chickwts)
# Cramer-von Mises
Cramer.test <- function(x, y, R=999){
  m <- length(x)
  n <- length(y)
  z <- c(x,y)
  N <- m + n 
  Fm <- numeric(N)
  Gn <- numeric(N)
  for(i in 1:N){
    Fm[i] <- mean(as.integer(z[i] <= x))
    Gn[i] <- mean(as.integer(z[i] <= y))
  }
  Cramer0 <- ((m*n)/N) * sum((Fm-Gn)^2)
  Cramer <- replicate(R, expr = {
    k <- sample(1:N)
    Z <- z[k]
    X <- Z[1:n]
    Y <- Z[(n+1):N]
    for(i in 1:N){
      Fm[i] <- mean(as.integer(Z[i] <= X))
      Gn[i] <- mean(as.integer(Z[i] <= Y))
    }
    ((m*n)/N) * sum((Fm - Gn)^2)
  })
  Cramer1 <- c(Cramer, Cramer0)
  return(list(statistic = Cramer0, p.value = mean(Cramer1>=Cramer0)))
}
# Apply test
soybean <- as.vector(weight[feed == "soybean"])
sunflower <- as.vector(weight[feed == "sunflower"])
linseed <- as.vector(weight[feed == "linseed"])
Cramer.test(soybean, linseed)
Cramer.test(sunflower, linseed)
```

## Exercise 8.3

The Count 5 test for equal variances in Section 6.4 is based on the maximum number of extreme points. Example 6.15 shows that the Count 5 criterion is not applicable for unequal sample sizes. Implement a permutation test for equal variance based on the maximum number of extreme points that applies when sample sizes are not necessarily equal.

```{r}
# Max Outliers
maxol <- function(x,y){
  X <- x - mean(x)
  Y <- y - mean(y)
  xout <- sum(X > max(Y)) + sum(X < min(Y))
  yout <- sum(Y > max(X)) + sum(Y < min(X))
  return(max(c(xout,yout)))
}

maxol.test <- function(x, y, R = 999){
  z <- c(x, y)
  n <- length(x)
  N <- length(z)
  maxout <- maxol(x,y)
  maxouts <- replicate(R, expr={
    k <- sample(1:N)
    k1 <- k[1:n]
    k2 <- k[(n+1):N]
    maxol(z[k1],z)
  })
  maxouts1 <- c(maxouts, maxout)
  return(list(estimate = maxout, p = mean(maxouts1 >= maxout)))
}

# Apply Test
## equal variance
x <- rnorm(10, 0, 1)
y <- rnorm(50, 0, 1)
maxol.test(x,y)
## unequal variance
x <- rnorm(10, 0, 1)
y <- rnorm(50, 0, 2)
maxol.test(x,y)
```

# Homework 7

## In-class Exercise

Consider a model $P(Y = 1 | X_1, X_2, X_3) = \frac{\exp(a+b_1X_1+b_2X_2+b_3X_3)}{1+\exp(a+b_1X_1+b_2X_2+b_3X_3)}$, where $X_1\sim P(1), X_2 \sim Exp(1)$ and $X_3 \sim B(1, 0.5)$.

• Design a function that takes as input values $N, b_1, b_2, b_3$ and $f_0$, and produces the output $a$.

• Call this function, input values are $N = 106, b_1 = 0, b_2 = 1, b_3 = −1, f_0 = 0.1, 0.01, 0.001, 0.0001.$
• Plot $− \log f_0$ vs $a$.

**Sol:** Using the uniroot method,

```{r}
sol_alpha <- function(N, b1, b2, b3, f0){
  x1 <- rpois(N,1)
  x2 <- rexp(N)
  x3 <- sample(0:1, N, replace = TRUE)
  g <- function(alpha){
    tmp <- exp(-alpha-b1*x1-b2*x2-b3*x3);
    p <- 1/(1+tmp)
    mean(p)-f0
  }
  solution <- uniroot(g, c(-50,0))
  return(solution$root)
}

N <- 106
b1 <- 0
b2 <- 1
b3 <- -1
f0 <- c(0.1, 0.01, 0.001, 0.0001)
alpha <- numeric(length(f0))
for(i in 1:length(f0)){
  alpha[i] <- sol_alpha(N, b1, b2, b3, f0[i])
}

plot(-log10(f0),alpha, type = 'l')

```

## 
Exercises 9.4 

Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of each chain.

**Sol:** Given that the standard Laplace distribution with $f(x) = \frac{1}{2}\exp(-|x|)$,

$$
\alpha(x_t,y) = \frac{f(y)}{f(x_t)}=\exp(|x_t|-|y|)
$$

```{r}
# random walk Metropolis Sampler
Metropolis_RW <- function(N, x_init, sigma){
  x <- numeric(N)
  x[1] <- x_init
  # rej
  u <- runif(N)
  cnt <- 0
  for(i in 2:N){
    xt <- x[i-1]
    y <- rnorm(1, xt, sigma)
    if(exp(abs(xt)-abs(y))>=u[i]) x[i] <- y
    else{
      x[i] <- x[i-1]
      cnt <- cnt + 1
    }
  }
   return(list(x = x, cnt = cnt))
}

N <- 10000
sigma <- c(1, 2, 4, 8)
x0 <- 0
for(i in 1:length(sigma)){
  RW <- Metropolis_RW(N, x0, sigma[i])
  # rejection rate
  res <- list(sigma = sigma[i], count = RW$cnt, acc_rate = 1-RW$cnt/N)
  print(unlist(res))
}
```

## Exercise 9.7 

Implement a Gibbs sampler to generate a bivariate normal chain $(X_t, Y_t)$ with zero means, unit standard deviations, and correlation 0.9. Plot the generated sample after discarding a suitable burn-in sample. Fit a simple linear regression model $Y = β_0 + β_1X$ to the sample and check the residuals of the model for normality and constant variance.

**Sol:**

```{r}
N <- 10000
burn_in <- 1000
# Bivariate Chain
X <- matrix(0, nrow = N, ncol = 2)
# init
X[1,] <- c(0,0)
# Params
mu1 <- mu2 <- 0
corr <- 0.9
sigma1 <- sigma2 <- 1
for(i in 2:N){
  yt <- X[i-1,2]
  mu <- mu1 + corr*(yt-mu2)*sigma1/sigma2
  X[i,1] <- rnorm(1, mean = mu, sd = sqrt(1-corr^2)*sigma1)
  xt <- X[i,1]
  mu <- mu2 + corr*(xt-mu1)*sigma2/sigma1
  X[i,2] <- rnorm(1, mean = mu, sd = sqrt(1-corr^2)*sigma2)
}
X_burned <- X[(burn_in+1):N,]
Xt <- X_burned[,1]
Yt <- X_burned[,2]
L <- lm(Yt~Xt)
# Check all required properties
L
summary(L)
# plot all samples
plot(Xt, Yt, cex = 0.025)
abline(h=0, v=0)

```

## Exercise 9.10

Refer to Example 9.1. Use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until the chain has converged approximately to the target distribution according to $\hat{R} < 1.2$. (See Exercise 9.9.) Also use the coda [212] package to check for convergence of the chain by the Gelman-Rubin method. Hints: See the help topics for the coda functions gelman.diag, gelman.plot, as.mcmc, and mcmc.list.

```{r}
library(coda)
# Rayleigh
Rayle <- function(x, sigma){
  if(any(x<0)) return(0)
  stopifnot(sigma>0)
  return((x/sigma^2)*exp(-x^2/(2*sigma^2)))
}
# M-H Chain
MH <- function(N, sigma, x_init){
  x <- numeric(N)
  x[1] <- x_init
  u <- runif(N)
  for(i in 2:N){
    xt <- x[i-1]
    y <- rchisq(1, df = xt)
    part1 <- Rayle(y, sigma) * dchisq(xt, df = y)
    part2 <- Rayle(xt, sigma) * dchisq(y, df = xt)
    if(part1/part2 >= u[i]) x[i] <- y
    else x[i] <- xt
  }
  return(x)
}

sigma <- 4
x_init <- rchisq(4, df=1)
N <- 1000
X <- matrix(0, nrow = length(x_init), ncol = N)
for(i in 1:length(x_init)) X[i,]<- MH(N, sigma, x_init[i])
Y <- mcmc.list(as.mcmc(X[1,]), as.mcmc(X[2,]), as.mcmc(X[3,]), as.mcmc(X[4,]))
print(gelman.diag(Y))

gelman.plot(Y)


```

# Homework 8

## In-class Exercise

Suppose some kind of disease is obedient with $X_1,\cdots, X_n\overset{i.i.d}{\sim} Exp(\lambda)$. Due to the characteristic of medical check-up, we can only determine $X_i$ fallen into some interval $(u_i, v_i)$, whose $u_i < v_i$ are fixed timestamp. This kind of data is called interval-censored data.

1)  Use two approaches to solve the MLE of $\lambda$, including directly solving the likelihood function and applying the EM algorithm. Then prove the convergence of EM process to the true MLE with a linear rate.

2)  Consider a series of observations of $(u_i, v_i)$, then implement the above two algorithms and reach the numerical solution of the MLE of $\lambda$. $(u_i, v_i): (11,12), (8,9), (27,28), (13,14), (16,17), (0,1), (23,24), (10,11), (24,25), (2,3)$

**Sol:**

1)  Since the log-likelihood function has the form:

$$
L(\lambda) =\Pi_i (P_\lambda(u_i\leq X_i\leq v_i)) = \Pi_i(\exp(-\lambda v_i)-\exp(-\lambda u_i))\\
\frac{\partial \log L(\lambda)}{\partial \lambda} = \sum_i\frac{u_i\exp(-\lambda u_i)-v_i\exp(-\lambda v_i)}{\exp(-\lambda v_i)-\exp(-\lambda u_i)}=?
$$

Since the closed-form solution of MLE is unreachable, thus realizing the EM Alg and compare it with numerical methods：

E-step： $$ E[X_i | X_i \in (u_i, v_i)] = \frac{\int_{u_i}^{v_i} x \lambda e^{-\lambda x} dx}{\int_{u_i}^{v_i} \lambda e^{-\lambda x} dx} $$

M-step，the log-likelihood is： $$ \log L(\lambda) = \sum_{i=1}^n \log(\lambda e^{-\lambda x_i}) = n \log(\lambda) - \lambda \sum_{i=1}^n x_i $$

Replace $x_i$ with $E[X_i | X_i \in (u_i, v_i)]$： $$ \log L(\lambda) = n \log(\lambda) - \lambda \sum_{i=1}^n E[X_i | X_i \in (u_i, v_i)] $$

Thus, the optim of $\lambda$ become： $$ \frac{d}{d\lambda} \log L(\lambda) = \frac{n}{\lambda} - \sum_{i=1}^n E[X_i | X_i \in (u_i, v_i)] = 0\\
 \lambda = \frac{n}{\sum_{i=1}^n E[X_i | X_i \in (u_i, v_i)]} $$

```{r}
library(stats4)

# All intervals
interval <- matrix(c(11, 12, 8, 9, 27, 28, 13, 14, 16, 17, 0, 1, 23, 24, 10, 11, 24, 25, 2, 3), ncol = 2, byrow = TRUE)

# E-step
E_step <- function(lambda, interval) {
  sapply(1:nrow(interval), function(i) {
    ui <- interval[i, 1]
    vi <- interval[i, 2]
    (integrate(function(x) x * lambda * exp(-lambda * x), lower = ui, upper = vi)$value) / 
    (integrate(function(x) lambda * exp(-lambda * x), lower = ui, upper = vi)$value)
  })
}

# M-step
M_step <- function(lambda, interval) {
  expect <- E_step(lambda, interval)
  n <- nrow(interval)
  lambda_upd <- n / sum(expect)
  return(lambda_upd)
}

# EM
EMproc <- function(interval, tolerance = 1e-6, max_iter = 1000) {
  lambda <- 10
  for (i in 1:max_iter) {
    lambda_upd <- M_step(lambda, interval)
    if (abs(lambda_upd - lambda) < tolerance) {
      break
    }
    lambda <- lambda_upd
  }
  return(lambda)
}

# negloglike
negloglike <- function(lambda, interval) {
  likelihood <- sapply(1:nrow(interval), function(i) {
    ui <- interval[i, 1]
    vi <- interval[i, 2]
    p <- integrate(function(x) lambda * exp(-lambda * x), lower = ui, upper = vi, rel.tol = 1e-8, abs.tol = 1e-10)$value
    max(p, .Machine$double.eps)
  })
  -sum(log(likelihood))
}
# maximize with "optim"
optimlambda <- function(interval) {
  result <- optim(par = 1, fn = negloglike, interval = interval, method = "L-BFGS-B", lower = 1e-2, upper = 1)
  return(result$par)
}

# EM Apply
lambda <- EMproc(interval)
print(lambda)

# Negloglike Apply
lambda <- optimlambda(interval)
print(lambda)


```

## Exercise 11.8

In the Morra game, the set of optimal strategies are not changed if a constant is subtracted from every entry of the payoff matrix, or a positive constant is multiplied times every entry of the payoff matrix. However, the simplex algorithm may terminate at a different basic feasible point (also optimal). Compute `B <- A + 2`, find the solution of game $B$, and verify that it is one of the extreme points (11.12)--(11.15) of the original game $A$. Also find the value of game $A$ and game $B$.

**Sol:** Imitate the Morra Game function in textbook,

```{r}
solve.game <- function(A) {
#solve the two player zero-sum game by simplex method
#optimize for player 1, then player 2
#maximize v subject to ...
#let x strategies 1:m, and put v as extra variable
#A1, the <= constraints
#
min.A <- min(A)
A <- A - min.A #so that v >= 0
max.A <- max(A)
A <- A / max(A)
m <- nrow(A)
n <- ncol(A)
it <- n^3
a <- c(rep(0, m), 1) #objective function
A1 <- -cbind(t(A), rep(-1, n)) #constraints <=
b1 <- rep(0, n)
A3 <- t(as.matrix(c(rep(1, m), 0))) #constraints sum(x)=1
b3 <- 1
sx <- simplex(a=a, A1=A1, b1=b1, A3=A3, b3=b3,
              maxi=TRUE, n.iter=it)
#the ’solution’ is [x1,x2,...,xm | value of game]
#
#minimize v subject to ...
#let y strategies 1:n, with v as extra variable
a <- c(rep(0, n), 1) #objective function
A1 <- cbind(A, rep(-1, m)) #constraints <=
b1 <- rep(0, m)
A3 <- t(as.matrix(c(rep(1, n), 0))) #constraints sum(y)=1
b3 <- 1
sy <- simplex(a=a, A1=A1, b1=b1, A3=A3, b3=b3,
maxi=FALSE, n.iter=it)
soln <- list("A" = A * max.A + min.A,
"x" = sx$soln[1:m],
"y" = sy$soln[1:n],
"v" = sx$soln[m+1] * max.A + min.A)
soln
}

# payoff matrix A
A <- matrix(c(0, -2, -2, 3, 0, 0, 4, 0, 0, 2, 0, 0, 0, -3, -3, 4, 0, 0, 2, 0, 0, 3, 0, 0, 0, -4, -4, -3, 0, -3, 0, 4, 0, 0, 5, 0, 0, 3, 0, -4, 0, -4, 0, 5, 0, 0, 3, 0, 0, 4, 0, -5, 0, -5, -4, -4, 0, 0, 0, 5, 0, 0, 6, 0, 0, 4, -5, -5, 0, 0, 0, 6, 0, 0, 4, 0, 0, 5, -6, -6, 0), nrow = 9, ncol = 9)

library(boot)

B <- A + 2
sol <- solve.game(B)
sol$v
round(cbind(sol$x, sol$y), 7)
round(sol$x * 61, 7)

```

The value of the game become $v = 2$ , while the solutions obtained by simplex method also correspond to (11.15).

# Homework 9

## 1.  Advanced R

### • 2.1.3 Exercise 4

Why do you need to use `unlist()` to convert a list to an atomic vector? Why doesn't `as.vector()` work?

**Sol:** The `unlist()` function converts a list into an atomic vector by concatenating its elements, while the `as.vector()` function does not perform such operation.

```{r}
mylist <- list(1, 2, 3)
my_unlist <- unlist(mylist)
my_asvector <- as.vector(mylist)
print(my_unlist)
print(my_asvector)
```

### • 2.3.1 Exercise 1, 2

1.  What does dim() return when applied to a vector?

    **Sol:** `NULL`

    ```{r}
    my_vector <- c(1,2,3)
    dim(my_vector)
    ```

2.  If is.matrix(x) is TRUE, what will is.array(x) return?

    **Sol:** `TRUE`

    ```{r}
    my_matrix <- matrix(1:4, nrow = 2, ncol = 2)
    is.matrix(my_matrix)
    is.array(my_matrix)
    ```

### • 2.4.5 Exercise 2, 3

2\. What does as.matrix() do when applied to a data frame with columns of different types?

**Sol:** All columns has been converted to `char` type.

```{r}
df <- data.frame(
  num = c(1, 2, 3),
  char = c("a", "b", "c"),
  bool = c(TRUE, FALSE, TRUE)
)
my_matrix <- as.matrix(df)
print(my_matrix)
```

3.  Can you have a data frame with 0 rows? What about 0 columns?

**Sol:** Yes.

```{r}
df <- data.frame()
print(df)
nrow(df)
ncol(df)
```

### • Exercises 2 (Page 204)

The function below scales a vector so it falls in the range [0,1]. How would you apply it to every column of a data frame? How would you apply it to every numeric column in a data frame? **Sol:** Use `lapply` function,

```{r}
scale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}
# data frame
df <- data.frame(
  A = c(1, 2, 3),
  B = c(1, 4, 9),
  C = c(1, 10, 100)
)
# Apply scale01() to every column
scaled_df <- as.data.frame(lapply(df, scale01))
print(scaled_df)

```

### • Exercises 1 (Page 213)

Use `vapply()` to:

a)  Compute the standard deviation of every column in a numeric data frame.

b)  Compute the standard deviation of every numeric column in a mixed data frame.

(Hint: you'll need to use vapply() twice.)

```{r}
df <- data.frame(
  A = c(1, 2, 3),
  B = c(1, 4, 9),
  C = c(1, 10, 100)
)
print(df)
# Compute the standard deviation
std_dev <- vapply(df, sd, numeric(1))
print(std_dev)

mix_df <- data.frame(
  A = c(1, 2, 3),
  B = c("A", "B", "C"),
  C = c(TRUE, FALSE, FALSE),
  D = c(1, 4, 9),
  E = c(1, 10, 100)
)
print(mix_df)
mix_numeric <- vapply(mix_df,function(x) all(is.numeric(x)),logical(1))
# Compute the standard deviation
std_dev <- vapply(mix_df[, mix_numeric], sd, numeric(1))
print(std_dev)

```

## 2.

Consider Exercise 9.8. (Hint: Refer to the first example of Case studies section)

**Exercise 9.8:** This example appears in [40]. Consider the bivariate density

$$
f(x,y)\propto (^n_x)y^{x+a-1}(1-y)^{n-x+b-1}, x=0,1,\cdots,n,0\leq y\leq 1
$$
It can be shown (see e.g. [23]) that for fixed $a, b, n$, the conditional distributions are Binomial$(n, y)$ and Beta$(x + a, n − x + b)$. Use the Gibbs sampler to generate a chain with target joint density $f(x, y)$.

• Write an R function.

• Write an Rcpp function.

• Compare the computation time of the two functions with the function "microbenchmark".

**Sol:** Firstly, write an R function,

```{r}
GibbsSamp <- function(n, a, b, iter) {
  # Init
  chain <- matrix(0, nrow = iter, ncol = 2)
  x <- 0
  y <- 0.5
  
  for (i in 1:iter) {
    # Update x from conditional B(n, y)
    x <- rbinom(1, n, y)
    
    # Update y from conditional Beta(x + a, n - x + b)
    y <- rbeta(1, x + a, n - x + b)
    
    chain[i, 1] <- x
    chain[i, 2] <- y
  }
  return(chain)
}
```

Then, write a Rcpp function

```{r, engine='Rcpp'}
#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
NumericMatrix GibbsSampCpp(int n, int a, int b, int iter) {
  // Init
  NumericMatrix chain(iter, 2);
  int x = 0;
  double y = 0.5;
  
  for (int i = 0; i < iter; i++) {
    // Update x from B(n, y)
    x = R::rbinom(n, y);
    
    // Update y from Beta(x + a, n - x + b)
    y = R::rbeta(x + a, n - x + b);
    
    chain(i, 0) = x;
    chain(i, 1) = y;
  }
  
  return chain;
}
```

Compare the two functions with `microbenchmark`

```{r}
library(microbenchmark)

n <- 100
a <- 2
b <- 3
iter <- 1000

# R function
r_time <- microbenchmark(
  GibbsSamp(n, a, b, iter),
  times = 10
)

# Rcpp function
cpp_time <- microbenchmark(
  GibbsSampCpp(n, a, b, iter),
  times = 10
)

# Print the computation time of both functions
print(r_time)
print(cpp_time)
```

In summary, based on the execution time results, the Rcpp algorithm (GibbsSampCpp) exhibits higher execution efficiency and is capable of completing the same task faster than the R algorithm (GibbsSamp).



