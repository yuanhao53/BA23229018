<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Yuanhao Pu" />

<meta name="date" content="2023-12-08" />

<title>Illustration</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Illustration</h1>
<h4 class="author">Yuanhao Pu</h4>
<h4 class="date">2023-12-08</h4>



<p>To TA:</p>
<p>Considering that my current research focus is mainly on artificial
intelligence algorithms based on deep learning, I have had limited
opportunities to use R as my coding language. Initially, I intended to
implement the whole definition for neural networks and the training
process for deep models (like CNN, VAE, etc.) from scratch in R.
Unfortunately, the lack of comprehensive support for <code>class</code>
in Rcpp within R packages resulted in significant time investment
without a viable solution. Consequently, due to time constraints, I had
to start afresh, ultimately implementing a BP algorithm for MLP model in
<code>R</code> and several optimization algorithms only for simple
linear models in <code>Rcpp</code>. I apologize for not achieving a very
satisfying project, and I would appreciate if you understand the
challenges and efforts I encountered in this regard.</p>
<div id="backpropagation-bp" class="section level2">
<h2>BackPropagation (BP)</h2>
<p>My package contains a whole training process of a basic Neural
Network, including the Backpropagation (BP) algorithm for a simple
Multi-Layer Perceptron (MLP) model.</p>
<div id="parameters" class="section level3">
<h3>Parameters</h3>
<p><code>X</code>: the feature dataset.</p>
<p><code>y</code>: the target label.</p>
<p><code>hidden_size</code>: the width of hidden layer</p>
<p><code>lr</code>: learning rate</p>
<p><code>epochs</code>: number of epochs to be trained</p>
<p><code>lambda</code>: regularization coefficient</p>
</div>
<div id="usage-example" class="section level3">
<h3>Usage Example</h3>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>train_inputs <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>), <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>train_targets <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="fu">mlp_train</span>(train_inputs, train_targets, <span class="at">hidden_size =</span> <span class="dv">4</span>, <span class="at">lr =</span> <span class="fl">0.1</span>, <span class="at">epochs =</span> <span class="dv">1000</span>, <span class="at">lambda =</span> <span class="fl">0.01</span>)</span></code></pre></div>
</div>
<div id="algorithm-flow" class="section level3">
<h3>Algorithm Flow</h3>
<ol style="list-style-type: decimal">
<li><p><strong>Initialization of Weights and Biases:</strong> Random
initialization of weights and biases from the input layer to the hidden
layer (<strong><code>weights_input_hidden</code></strong>,
<strong><code>biases_hidden</code></strong>), and from the hidden layer
to the output layer
(<strong><code>weights_hidden_output</code></strong>,
<strong><code>biases_output</code></strong>).</p></li>
<li><p><strong>Forward Propagation:</strong> For each input sample,
perform forward propagation to compute the outputs of the hidden layer
and the output layer.</p></li>
<li><p><strong>Loss Calculation:</strong> Utilize Mean Squared Error
(MSE) as the loss function to compute the error between predicted output
and actual labels.</p></li>
<li><p><strong>Backward Propagation (BP):</strong> Update weights and
biases based on the computed gradients, aiming to minimize the
loss.</p></li>
<li><p><strong>Iterative Training:</strong> Repeat the above steps for a
specified number of training epochs.</p></li>
<li><p><strong>Loss Graph Plotting:</strong> Record the loss at each
epoch and plot the variation of loss over time.</p></li>
</ol>
</div>
</div>
<div id="gradient-descent-gd" class="section level2">
<h2>Gradient Descent (GD)</h2>
<p>Gradient Descent is a widely used optimization algorithm for finding
the minimum of a function, often employed in machine learning for
optimizing the parameters of a model.</p>
<p>Gradient Descent is based on the observation that if the
multi-variable function <span class="math inline">\(F(x)\)</span> is
well-defined and differentiable in a neighborhood of a point <span class="math inline">\(a\)</span>, then <span class="math inline">\(F(x)\)</span> decreases <strong>fastest</strong>
if one goes from <span class="math inline">\(a\)</span> in the direction
of the negative gradient of <span class="math inline">\(F\)</span> at
<span class="math inline">\(a, -\nabla F(a)\)</span>. It follows that,
if</p>
<p><span class="math display">\[
a_{n+1} = a_n-\gamma\nabla F(a_n)
\]</span></p>
<p>for a small enough step size or learning rate <span class="math inline">\(\gamma\in\mathbb{R}_+\)</span>, then <span class="math inline">\(F(a_n)\geq F(a_{n+1})\)</span>. In other words,
the term <span class="math inline">\(\gamma\nabla F(a)\)</span> is
subtracted from <span class="math inline">\(a\)</span> because we want
to move against the gradient, toward the local minimum. With this
observation in mind, one starts with a guess <span class="math inline">\(x_0\)</span> for a local minimum of <span class="math inline">\(F\)</span>, and considers the sequence <span class="math inline">\(x_0, x_1, x_2,\cdots\)</span> such that</p>
<p><span class="math display">\[
x_{n+1} = x_n-\gamma\nabla F(x_n), n\geq 0
\]</span></p>
<p>We have a monotonic sequence</p>
<p><span class="math display">\[
F(x_0)\geq F(x_1)\geq F(x_2)\geq \cdots
\]</span></p>
<p>so, hopefully, the sequence <span class="math inline">\((x_n)\)</span> converges to the desired local
minimum. Note that the value of the <strong>step size</strong> <span class="math inline">\(\gamma\)</span> is allowed to change at every
iteration.</p>
<div id="parameters-1" class="section level3">
<h3>Parameters</h3>
<p><code>X</code>: The design matrix of size M*N.</p>
<p><code>y</code>: The dependent vector of size M*1.</p>
<p><code>lr</code>: The learning rate.</p>
<p><code>num_iters</code>: The number of iterations.</p>
<p>Return: The vector of optimal parameter values.</p>
</div>
<div id="usage-example-1" class="section level3">
<h3>Usage Example</h3>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># Example usage of gradient_descent function</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">1000</span>), <span class="at">ncol =</span> <span class="dv">20</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">rbinom</span>(<span class="dv">50</span>, <span class="dv">1</span>, <span class="fl">0.5</span>) <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>lr <span class="ot">&lt;-</span> <span class="fl">0.0001</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>num_iters <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">gradient_descent</span>(X, y, lr, num_iters)</span></code></pre></div>
</div>
<div id="algorithm-flow-1" class="section level3">
<h3>Algorithm Flow</h3>
<ul>
<li><p>Initialization: Initialize parameters (theta) to zero or random
values.</p></li>
<li><p>Iterative Update:</p></li>
</ul>
<p>– For each iteration, compute the error by predicting the output
using the current parameters.</p>
<p>– Compute the gradient of the loss with respect to each
parameter.</p>
<p>– Update each parameter by subtracting the learning rate multiplied
by the gradient.</p>
<ul>
<li>Repeat the iterative update until the specified number of iterations
is reached.</li>
</ul>
<p>This algorithm gradually minimizes the loss function by iteratively
adjusting the parameters based on the computed gradients.</p>
<p>Note that my package only provide a very basic implementation with
linear models. In practice, the loss function as well as hyperparams
fine-tuning can be adjusted.</p>
</div>
</div>
<div id="stochastic-gradient-descent-sgd" class="section level2">
<h2>Stochastic Gradient Descent (SGD)</h2>
<p>Stochastic Gradient Descent is a variant of the gradient descent
algorithm that updates the parameters using a single randomly chosen
sample at each iteration. This helps reduce computational costs,
especially for large datasets.</p>
<p>Most of SGD flows are the same with GD process, the main difference
is that:</p>
<p>For each iteration, SGD randomly select a single sample, then compute
the error by predicting the output using the current parameters and the
selected sample and update the parameter.</p>
<p>This algorithm updates the parameters based on the gradient of a
single sample, making it computationally efficient for large datasets.
Correspondingly, SGD suffers from a lower stability.</p>
<div id="usage-example-2" class="section level3">
<h3>Usage Example</h3>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># Example usage of stochastic_gradient_descent function</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">1000</span>), <span class="at">ncol =</span> <span class="dv">20</span>)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">rbinom</span>(<span class="dv">50</span>, <span class="dv">1</span>, <span class="fl">0.5</span>) <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>lr <span class="ot">&lt;-</span> <span class="fl">0.0001</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>num_iters <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">stochastic_gradient_descent</span>(X, y, lr, num_iters)</span></code></pre></div>
</div>
</div>
<div id="minibatch-gd" class="section level2">
<h2>Minibatch-GD</h2>
<p>Mini-Batch Gradient Descent is a compromise between Gradient Descent
and Stochastic Gradient Descent. It updates the parameters using a small
batch of randomly chosen samples at each iteration, balancing
computational efficiency and convergence stability.</p>
<div id="added-parameters" class="section level3">
<h3>Added Parameters</h3>
<p><strong><code>batch_size</code></strong>: The size of each
mini-batch.</p>
</div>
<div id="usage-example-3" class="section level3">
<h3>Usage Example</h3>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="co"># Example usage of minibatch_gradient_descent function</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">1000</span>), <span class="at">ncol =</span> <span class="dv">20</span>)</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">rbinom</span>(<span class="dv">50</span>, <span class="dv">1</span>, <span class="fl">0.5</span>) <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>lr <span class="ot">&lt;-</span> <span class="fl">0.0001</span></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>batch_size <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>num_iters <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">minibatch_gradient_descent</span>(X, y, lr, batch_size, num_iters)</span></code></pre></div>
</div>
</div>
<div id="sgd-with-momentum-sgd-m-optimization" class="section level2">
<h2>SGD with Momentum (SGD-M) Optimization</h2>
<p>SGD with Momentum introduces the concept of momentum to Stochastic
Gradient Descent, helping to accelerate training and reduce oscillations
by considering a weighted average of previous gradients.</p>
<p>SGD-M remembers the update <span class="math inline">\(\Delta
w\)</span> at each iteration, and determines the next update as a linear
combination of the gradient and the previous update:</p>
<p><span class="math display">\[
\Delta w:=\alpha\Delta w-\eta\nabla Q_i(w)\\
w:=w+\Delta w
\]</span></p>
<p>that leads to</p>
<p><span class="math display">\[
w:=w-\eta\nabla Q_i(w)+\alpha\Delta w
\]</span></p>
<p>where the parameter <span class="math inline">\(w\)</span> which
minimizes <span class="math inline">\(Q(w)\)</span> is to be estimated,
<span class="math inline">\(\eta\)</span> is the learning-rate and <span class="math inline">\(\alpha\)</span> is an exponential decay factor
between 0 and 1 that determines the relative contribution of the current
gradient and earlier gradients to the weight change.</p>
<div id="added-parameters-1" class="section level3">
<h3>Added Parameters</h3>
<p><strong><code>beta</code></strong>: The momentum factor.</p>
</div>
<div id="usage-example-4" class="section level3">
<h3>Usage Example</h3>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># Example usage of sgd_with_momentum function</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">1000</span>), <span class="at">ncol =</span> <span class="dv">20</span>)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">rbinom</span>(<span class="dv">50</span>, <span class="dv">1</span>, <span class="fl">0.5</span>) <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>lr <span class="ot">&lt;-</span> <span class="fl">0.0001</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fl">0.9</span></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>num_iters <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">sgd_with_momentum</span>(X, y, lr, beta, num_iters)</span></code></pre></div>
</div>
<div id="algorithm-flow-2" class="section level3">
<h3><strong>Algorithm Flow</strong></h3>
<ol style="list-style-type: decimal">
<li><p><strong>Initialization</strong>: Initialize parameters (theta) to
zero or random values.</p></li>
<li><p><strong>Initialize Momentum Variable</strong>: Initialize the
momentum variable (v) to zero.</p></li>
<li><p><strong>Iterative Update</strong>:</p>
<ul>
<li><p>For each iteration, compute the error by predicting the output
using the current parameters.</p></li>
<li><p>Compute the gradient of the loss with respect to each
parameter.</p></li>
<li><p>Update the momentum variable:
<code>v = beta * v + (1 - beta) * gradient</code>.</p></li>
<li><p>Update each parameter using the momentum:
<code>theta = theta - lr * v</code>.</p></li>
</ul></li>
<li><p>Repeat the iterative update until the specified number of
iterations is reached.</p></li>
</ol>
<p>This algorithm accelerates convergence and reduces oscillations by
incorporating a momentum term in the parameter updates.</p>
</div>
</div>
<div id="adam" class="section level2">
<h2>Adam</h2>
<p>Adam is an adaptive learning rate optimization algorithm that
combines ideas from Momentum and RMSProp. It utilizes first and second
moment estimates of the gradients, with bias correction, to improve
stability and convergence.</p>
<div id="added-parameters-2" class="section level3">
<h3>Added Parameters</h3>
<p><strong><code>beta1</code></strong>: The exponential decay factor for
the first moment.</p>
<p><strong><code>beta2</code></strong>: The exponential decay factor for
the second moment.</p>
<p><strong><code>epsilon</code></strong>: A small constant to avoid
division by zero.</p>
</div>
<div id="usage-example-5" class="section level3">
<h3>Usage Example</h3>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co"># Example usage of adam_optimization function</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">1000</span>), <span class="at">ncol =</span> <span class="dv">20</span>)</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">rbinom</span>(<span class="dv">50</span>, <span class="dv">1</span>, <span class="fl">0.5</span>) <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>lr <span class="ot">&lt;-</span> <span class="fl">0.0001</span></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>beta1 <span class="ot">&lt;-</span> <span class="fl">0.9</span></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>beta2 <span class="ot">&lt;-</span> <span class="fl">0.999</span></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>epsilon <span class="ot">&lt;-</span> <span class="fl">1e-8</span></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>num_iters <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">adam_optimization</span>(X, y, lr, beta1, beta2, epsilon, num_iters)</span></code></pre></div>
</div>
<div id="algorithm-flow-3" class="section level3">
<h3>Algorithm Flow</h3>
</div>
</div>
<div id="algorithm-flow-4" class="section level2">
<h2><strong>Algorithm Flow</strong></h2>
<ol style="list-style-type: decimal">
<li><p><strong>Initialization</strong>: Initialize parameters (theta) to
zero or random values.</p></li>
<li><p><strong>Initialize Moment and Second Moment Variables</strong>:
Initialize first moment estimate (m) and second moment estimate (v) to
zero.</p></li>
<li><p><strong>Initialize Time Step Variable</strong>: Initialize time
step variable (t) to 1.</p></li>
<li><p><strong>Iterative Update</strong>:</p>
<ul>
<li><p>For each iteration, compute the error by predicting the output
using the current parameters.</p></li>
<li><p>Compute the gradient of the loss with respect to each
parameter.</p></li>
<li><p>Update first moment estimate:
<code>m = beta1 * m + (1 - beta1) * gradient</code>.</p></li>
<li><p>Update second moment estimate:
<code>v = beta2 * v + (1 - beta2) * gradient**2</code>.</p></li>
<li><p>Correct bias in first moment:
<code>m_hat = m / (1 - beta1 ** t)</code>.</p></li>
<li><p>Correct bias in second moment:
<code>v_hat = v / (1 - beta2 ** t)</code>.</p></li>
<li><p>Update each parameter: theta =
<code>theta - lr * m_hat / (sqrt(v_hat) + epsilon)</code>.</p></li>
<li><p>Update time step:<code>t = t + 1</code>.</p></li>
</ul></li>
<li><p>Repeat the iterative update until the specified number of
iterations is reached.</p></li>
</ol>
<p>This algorithm adapts the learning rates for each parameter based on
the estimates of first and second moments of the gradients.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
