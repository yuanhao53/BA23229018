---
title: "Illustration"
author: "Yuanhao Pu"
date: "2023-12-08"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Illustration}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

To TA:

Considering that my current research focus is mainly on artificial intelligence algorithms based on deep learning, I have had limited opportunities to use R as my coding language. Initially, I intended to implement the whole definition for neural networks and the training process for deep models (like CNN, VAE, etc.) from scratch in R. Unfortunately, the lack of comprehensive support for `class` in Rcpp within R packages resulted in significant time investment without a viable solution. Consequently, due to time constraints, I had to start afresh, ultimately implementing a BP algorithm for MLP model in `R` and several optimization algorithms only for simple linear models in `Rcpp`. I apologize for not achieving a very satisfying project, and I would appreciate if you understand the challenges and efforts I encountered in this regard.

## BackPropagation (BP)

My package contains a whole training process of a basic Neural Network, including the Backpropagation (BP) algorithm for a simple Multi-Layer Perceptron (MLP) model.

### Parameters

`X`: the feature dataset.

`y`: the target label.

`hidden_size`: the width of hidden layer

`lr`: learning rate

`epochs`: number of epochs to be trained

`lambda`: regularization coefficient

### Usage Example

```{r, eval = FALSE}
train_inputs <- matrix(c(0, 0, 0, 1, 1, 0, 1, 1), ncol = 2, byrow = TRUE)
train_targets <- c(0, 1, 1, 0)
mlp_train(train_inputs, train_targets, hidden_size = 4, lr = 0.1, epochs = 1000, lambda = 0.01)
```

### Algorithm Flow

1.  **Initialization of Weights and Biases:** Random initialization of weights and biases from the input layer to the hidden layer (**`weights_input_hidden`**, **`biases_hidden`**), and from the hidden layer to the output layer (**`weights_hidden_output`**, **`biases_output`**).

2.  **Forward Propagation:** For each input sample, perform forward propagation to compute the outputs of the hidden layer and the output layer.

3.  **Loss Calculation:** Utilize Mean Squared Error (MSE) as the loss function to compute the error between predicted output and actual labels.

4.  **Backward Propagation (BP):** Update weights and biases based on the computed gradients, aiming to minimize the loss.

5.  **Iterative Training:** Repeat the above steps for a specified number of training epochs.

6.  **Loss Graph Plotting:** Record the loss at each epoch and plot the variation of loss over time.

## Gradient Descent (GD)

Gradient Descent is a widely used optimization algorithm for finding the minimum of a function, often employed in machine learning for optimizing the parameters of a model.

Gradient Descent is based on the observation that if the multi-variable function $F(x)$ is well-defined and differentiable in a neighborhood of a point $a$, then $F(x)$ decreases **fastest** if one goes from $a$ in the direction of the negative gradient of $F$ at $a, -\nabla F(a)$. It follows that, if

$$
a_{n+1} = a_n-\gamma\nabla F(a_n)
$$

for a small enough step size or learning rate $\gamma\in\mathbb{R}_+$, then $F(a_n)\geq F(a_{n+1})$. In other words, the term $\gamma\nabla F(a)$ is subtracted from $a$ because we want to move against the gradient, toward the local minimum. With this observation in mind, one starts with a guess $x_0$ for a local minimum of $F$, and considers the sequence $x_0, x_1, x_2,\cdots$ such that

$$
x_{n+1} = x_n-\gamma\nabla F(x_n), n\geq 0
$$

We have a monotonic sequence

$$
F(x_0)\geq F(x_1)\geq F(x_2)\geq \cdots
$$

so, hopefully, the sequence $(x_n)$ converges to the desired local minimum. Note that the value of the **step size** $\gamma$ is allowed to change at every iteration.

### Parameters

`X`: The design matrix of size M\*N.

`y`: The dependent vector of size M\*1.

`lr`: The learning rate.

`num_iters`: The number of iterations.

Return: The vector of optimal parameter values.

### Usage Example

```{r, eval = FALSE}
# Example usage of gradient_descent function
X <- matrix(runif(1000), ncol = 20)
y <- 2 * rbinom(50, 1, 0.5) - 1
lr <- 0.0001
num_iters <- 100
theta <- gradient_descent(X, y, lr, num_iters)
```

### Algorithm Flow

-   Initialization: Initialize parameters (theta) to zero or random values.

-   Iterative Update:

-- For each iteration, compute the error by predicting the output using the current parameters.

-- Compute the gradient of the loss with respect to each parameter.

-- Update each parameter by subtracting the learning rate multiplied by the gradient.

-   Repeat the iterative update until the specified number of iterations is reached.

This algorithm gradually minimizes the loss function by iteratively adjusting the parameters based on the computed gradients.

Note that my package only provide a very basic implementation with linear models. In practice, the loss function as well as hyperparams fine-tuning can be adjusted.

## Stochastic Gradient Descent (SGD)

Stochastic Gradient Descent is a variant of the gradient descent algorithm that updates the parameters using a single randomly chosen sample at each iteration. This helps reduce computational costs, especially for large datasets.

Most of SGD flows are the same with GD process, the main difference is that:

For each iteration, SGD randomly select a single sample, then compute the error by predicting the output using the current parameters and the selected sample and update the parameter.

This algorithm updates the parameters based on the gradient of a single sample, making it computationally efficient for large datasets. Correspondingly, SGD suffers from a lower stability.

### Usage Example

```{r, eval = FALSE}
# Example usage of stochastic_gradient_descent function
X <- matrix(runif(1000), ncol = 20)
y <- 2 * rbinom(50, 1, 0.5) - 1
lr <- 0.0001
num_iters <- 100
theta <- stochastic_gradient_descent(X, y, lr, num_iters)

```

## Minibatch-GD

Mini-Batch Gradient Descent is a compromise between Gradient Descent and Stochastic Gradient Descent. It updates the parameters using a small batch of randomly chosen samples at each iteration, balancing computational efficiency and convergence stability.

### Added Parameters

**`batch_size`**: The size of each mini-batch.

### Usage Example

```{r, eval = FALSE}
# Example usage of minibatch_gradient_descent function
X <- matrix(runif(1000), ncol = 20)
y <- 2 * rbinom(50, 1, 0.5) - 1
lr <- 0.0001
batch_size <- 10
num_iters <- 100
theta <- minibatch_gradient_descent(X, y, lr, batch_size, num_iters)
```

## SGD with Momentum (SGD-M) Optimization

SGD with Momentum introduces the concept of momentum to Stochastic Gradient Descent, helping to accelerate training and reduce oscillations by considering a weighted average of previous gradients.

SGD-M remembers the update $\Delta w$ at each iteration, and determines the next update as a linear combination of the gradient and the previous update:

$$
\Delta w:=\alpha\Delta w-\eta\nabla Q_i(w)\\
w:=w+\Delta w
$$

that leads to

$$
w:=w-\eta\nabla Q_i(w)+\alpha\Delta w
$$

where the parameter $w$ which minimizes $Q(w)$ is to be estimated, $\eta$ is the learning-rate and $\alpha$ is an exponential decay factor between 0 and 1 that determines the relative contribution of the current gradient and earlier gradients to the weight change.

### Added Parameters

**`beta`**: The momentum factor.

### Usage Example

```{r, eval = FALSE}
# Example usage of sgd_with_momentum function
X <- matrix(runif(1000), ncol = 20)
y <- 2 * rbinom(50, 1, 0.5) - 1
lr <- 0.0001
beta <- 0.9
num_iters <- 100
theta <- sgd_with_momentum(X, y, lr, beta, num_iters)
```

### **Algorithm Flow**

1.  **Initialization**: Initialize parameters (theta) to zero or random values.

2.  **Initialize Momentum Variable**: Initialize the momentum variable (v) to zero.

3.  **Iterative Update**:

    -   For each iteration, compute the error by predicting the output using the current parameters.

    -   Compute the gradient of the loss with respect to each parameter.

    -   Update the momentum variable: `v = beta * v + (1 - beta) * gradient`.

    -   Update each parameter using the momentum: `theta = theta - lr * v`.

4.  Repeat the iterative update until the specified number of iterations is reached.

This algorithm accelerates convergence and reduces oscillations by incorporating a momentum term in the parameter updates.

## Adam

Adam is an adaptive learning rate optimization algorithm that combines ideas from Momentum and RMSProp. It utilizes first and second moment estimates of the gradients, with bias correction, to improve stability and convergence.

### Added Parameters

**`beta1`**: The exponential decay factor for the first moment.

**`beta2`**: The exponential decay factor for the second moment.

**`epsilon`**: A small constant to avoid division by zero.

### Usage Example

```{r, eval = FALSE}
# Example usage of adam_optimization function
X <- matrix(runif(1000), ncol = 20)
y <- 2 * rbinom(50, 1, 0.5) - 1
lr <- 0.0001
beta1 <- 0.9
beta2 <- 0.999
epsilon <- 1e-8
num_iters <- 100
theta <- adam_optimization(X, y, lr, beta1, beta2, epsilon, num_iters)
```

### Algorithm Flow

## **Algorithm Flow**

1.  **Initialization**: Initialize parameters (theta) to zero or random values.

2.  **Initialize Moment and Second Moment Variables**: Initialize first moment estimate (m) and second moment estimate (v) to zero.

3.  **Initialize Time Step Variable**: Initialize time step variable (t) to 1.

4.  **Iterative Update**:

    -   For each iteration, compute the error by predicting the output using the current parameters.

    -   Compute the gradient of the loss with respect to each parameter.

    -   Update first moment estimate: `m = beta1 * m + (1 - beta1) * gradient`.

    -   Update second moment estimate: `v = beta2 * v + (1 - beta2) * gradient**2`.

    -   Correct bias in first moment: `m_hat = m / (1 - beta1 ** t)`.

    -   Correct bias in second moment: `v_hat = v / (1 - beta2 ** t)`.

    -   Update each parameter: theta = `theta - lr * m_hat / (sqrt(v_hat) + epsilon)`.

    -   Update time step:`t = t + 1`.

5.  Repeat the iterative update until the specified number of iterations is reached.

This algorithm adapts the learning rates for each parameter based on the estimates of first and second moments of the gradients.
