# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' @name Adam
#' @title Adam Optimization
#' @description Adam optimization algorithm
#' @param X The design matrix of size M*N
#' @param y The dependent vector of size M*1
#' @param lr The learning rate
#' @param beta1 Exponential decay rate for the first moment estimate
#' @param beta2 Exponential decay rate for the second moment estimate
#' @param epsilon Small constant to avoid division by zero
#' @param num_iters The number of iterations
#' @return The vector of optimal parameter values
#' @examples
#' \dontrun{
#' X <- matrix(runif(1000), ncol = 20)
#' y <- 2*rbinom(50,1,0.5)-1
#' lr <- 0.001
#' beta1 <- 0.9
#' beta2 <- 0.999
#' epsilon <- 1e-8
#' num_iters <- 100
#' theta_adam <- adam_optimization(X, y, lr, beta1, beta2, epsilon, num_iters)
#' }
#' @export
adam_optimization <- function(X, y, lr, beta1, beta2, epsilon, num_iters) {
    .Call('_BA23229018_adam_optimization', PACKAGE = 'BA23229018', X, y, lr, beta1, beta2, epsilon, num_iters)
}

#' @name GD
#' @title Gradient Descent
#' @description A simple implementation of Gradient Descent optimization, here we use a linear regression model for example. Any other type of loss function is reachable.
#' @param X The design matrix of size M*N
#' @param y The dependent vector of size M*1
#' @param lr The learning rate
#' @param num_iters The number of iterations
#' @return The vector of optimal parameter values
#' @examples
#' \dontrun{
#' X <- matrix(runif(1000), ncol = 20)
#' y <- 2*rbinom(50,1,0.5)-1
#' lr <- 0.001
#' num_iters <- 100
#' theta <- gradient_descent(X, y, lr, num_iters)
#' }
#' @export
gradient_descent <- function(X, y, lr, num_iters) {
    .Call('_BA23229018_gradient_descent', PACKAGE = 'BA23229018', X, y, lr, num_iters)
}

#' @name MinibatchGD
#' @title Mini-Batch Gradient Descent
#' @description Mini-Batch Gradient Descent optimization algorithm
#' @param X The design matrix of size M*N
#' @param y The dependent vector of size M*1
#' @param lr The learning rate
#' @param num_iters The number of iterations
#' @param batch_size The size of mini-batch
#' @return The vector of optimal parameter values
#' @examples
#' \dontrun{
#' X <- matrix(runif(1000), ncol = 20)
#' y <- 2*rbinom(50,1,0.5)-1
#' lr <- 0.0001
#' num_iters <- 100
#' batch_size <- 10
#' theta_minibatch <- minibatch_gradient_descent(X, y, lr, num_iters, batch_size)
#' }
#' @export
minibatch_gradient_descent <- function(X, y, lr, num_iters, batch_size) {
    .Call('_BA23229018_minibatch_gradient_descent', PACKAGE = 'BA23229018', X, y, lr, num_iters, batch_size)
}

#' @name SGD
#' @title Stochastic Gradient Descent
#' @description Stochastic Gradient Descent optimization algorithm
#' @param X The design matrix of size M*N
#' @param y The dependent vector of size M*1
#' @param lr The learning rate
#' @param num_iters The number of iterations
#' @return The vector of optimal parameter values
#' @examples
#' \dontrun{
#' X <- matrix(runif(1000), ncol = 20)
#' y <- 2*rbinom(50,1,0.5)-1
#' lr <- 0.001
#' num_iters <- 100
#' theta_sgd <- stochastic_gradient_descent(X, y, lr, num_iters)
#' }
#' @export
stochastic_gradient_descent <- function(X, y, lr, num_iters) {
    .Call('_BA23229018_stochastic_gradient_descent', PACKAGE = 'BA23229018', X, y, lr, num_iters)
}

#' @name SGDM
#' @title Stochastic Gradient Descent with Momentum
#' @description Stochastic Gradient Descent optimization algorithm with momentum
#' @param X The design matrix of size M*N
#' @param y The dependent vector of size M*1
#' @param lr The learning rate
#' @param momentum The momentum parameter
#' @param num_iters The number of iterations
#' @return The vector of optimal parameter values
#' @examples
#' \dontrun{
#' X <- matrix(runif(1000), ncol = 20)
#' y <- 2*rbinom(50,1,0.5)-1
#' lr <- 0.0001
#' momentum <- 0.9
#' num_iters <- 100
#' theta_sgd_momentum <- sgd_momentum(X, y, lr, momentum, num_iters)
#' }
#' @export
sgd_momentum <- function(X, y, lr, momentum, num_iters) {
    .Call('_BA23229018_sgd_momentum', PACKAGE = 'BA23229018', X, y, lr, momentum, num_iters)
}

